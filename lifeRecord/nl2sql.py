# lifeRecord/NL2SQL.py
import openai
from pathlib import Path
import chromadb
import chromadb.utils.embedding_functions as embedding_functions
import os
import json
import pandas as pd
from tqdm import tqdm

BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
DB_PATH = BASE_DIR / "vectordb"

SCHEMA_FILE = DATA_DIR / "lifeRecordSchema.json"
FEWSHOT_FILE = DATA_DIR / "fewshot.jsonl"
API_KEY_FILE = DATA_DIR / "api_key.txt"

def load_api_key(api_key_path):
    with open(api_key_path, "r", encoding="utf-8") as f:
        return f.read().strip()

OPENAI_API_KEY = load_api_key(API_KEY_FILE)

def nlTosql(user_question):
    llm = openai.OpenAI(api_key=OPENAI_API_KEY)

    relevant_columns = find_relevant_columns(user_question, top_k=5)

    schema_info = generate_filtered_schema(
        input_file=SCHEMA_FILE,
        used_columns=relevant_columns,
        include_descriptions=True
    )

    fewshots_text = get_relevant_fewshots(user_question, top_k=3)

    prompt_template = """   
                        ë‹¹ì‹ ì€ í˜„ì¬ ìƒí™œê¸°ë¡ë¶€ë¥¼ ì‘ì„± ì¤‘ì¸ êµì‚¬ë¥¼ ë„ì™€ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì°¾ëŠ” ì¼ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.
                        ìƒí™œê¸°ë¡ë¶€(í•™ìƒìƒí™œê¸°ë¡ë¶€)ëŠ” ëŒ€í•œë¯¼êµ­ ì´ˆÂ·ì¤‘Â·ê³  í•™ìƒë“¤ì˜ í•™êµ ìƒí™œ ì „ë°˜ì„ ê³µì‹ì ìœ¼ë¡œ ê¸°ë¡í•˜ëŠ” ë¬¸ì„œë¡œ, 
                        êµì‚¬ë“¤ì´ ì‘ì„±í•˜ë©° ëŒ€í•™ ì…ì‹œë‚˜ ì·¨ì—… ì‹œ ì¤‘ìš”í•œ í‰ê°€ ìë£Œë¡œ í™œìš©ë©ë‹ˆë‹¤.
                        í•™ìƒì˜ í™œë™, êµê³¼ ì„±ì , ìˆ˜ìƒ, ë´‰ì‚¬ ë“± ëª¨ë“  ì •ë³´ê°€ ë¹ ì§ì—†ì´ ì˜ ë°˜ì˜ë˜ë„ë¡ ì ì ˆí•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

                        ì•„ë˜ì— ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ í˜¹ì€ ëª…ë ¹ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.
                        ìŠ¤í‚¤ë§ˆë¥¼ ê¼¼ê¼¼íˆ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ ì´í•´í•œ í›„, ìƒí™œê¸°ë¡ë¶€ ì‘ì„±ì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ì°¾ê¸° ìœ„í•œ ìµœì ì˜ SQLite ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
                        ì¿¼ë¦¬ ì‘ì„± ì „, ì‚¬ìš©ìê°€ ìš”êµ¬í•˜ëŠ” ì¡°ê±´(ì˜ˆ: í•™ë…„, í•™ê¸°, í™œë™ ìœ í˜•, ë™ì•„ë¦¬ëª…, ê³¼ëª©ëª… ë“±)ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•˜ë©° ì‚¬ê³  ê³¼ì •ì„ ì„œìˆ í•´ ì£¼ì„¸ìš”.

                        íŠ¹íˆ, ë‹¤ìŒ ì‚¬í•­ì„ ì£¼ì˜ ê¹Šê²Œ ê³ ë ¤í•˜ì„¸ìš”:
                        - ë‚ ì§œ(ì •í™•í•œ ë‚ ì§œ, ì—°ë„, í•™ê¸°): ì—°ë„ì™€ í•™ê¸°ë¡œ ì§ˆì˜ê°€ ë“¤ì–´ì˜¨ ê²½ìš° ë²”ìœ„ë¥¼ í†µí•´ í•´ë‹¹ ë‚ ì§œê°€ ê·¸ ë²”ìœ„ì— ë“¤ì–´ê°€ëŠ”ì§€ í™•ì¸í•´ì•¼í•©ë‹ˆë‹¤.
                            í™œë™ ë‚ ì§œì˜ ê²½ìš° "activity_date"ë¡œ í™•ì¸ê°€ëŠ¥í•©ë‹ˆë‹¤.
                            ex) 2024ë…„ 1í•™ê¸°ëŠ” ë‚ ì§œë¡œ 2024-03-01ì—ì„œ 2024-07-31 ì‚¬ì´ì˜ ê°’ì…ë‹ˆë‹¤.
                        - í™œë™ ìœ í˜• (ì˜ˆ: ë™ì•„ë¦¬ í™œë™, ììœ¨ í™œë™, ì§„ë¡œ í™œë™, íŠ¹ê¸° ì ì„±)
                        - íŠ¹ì • ë™ì•„ë¦¬ëª… (ì˜ˆ: í”¼íƒ€ê³ ë¼ìŠ¤)
                        - í•™ìƒ ê°œì¸ í˜¹ì€ ê·¸ë£¹ ëŒ€ìƒ ì—¬ë¶€

                        ì•„ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì…ë‹ˆë‹¤:
                        {DATABASE_SCHEMA}

                        ì•„ë˜ëŠ” ì˜ˆì‹œ ì§ˆë¬¸ ë° ì¿¼ë¦¬ì…ë‹ˆë‹¤:
                        {FEWSHOTS}

                        ì‚¬ìš©ì ì§ˆë¬¸ í˜¹ì€ ëª…ë ¹:
                        {QUESTION}

                        íŒíŠ¸:
                        {HINT}

                        ìµœì¢… ê²°ê³¼ëŠ” ë‹¤ìŒ JSON í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”:

                        {{
                        "chain_of_thought_reasoning": "ë‹¹ì‹ ì´ ìµœì¢… SQL ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•´ ë¶„ì„í•œ ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •",
                        "SQL": "ìµœì¢… ì‘ì„±í•œ SQLite ì¿¼ë¦¬ë¬¸"
                        }}

                        ì§ˆë¬¸ì— ë§ëŠ” ì •í™•í•˜ê³  ì™„ì „í•œ ì¿¼ë¦¬ë¥¼ ë‹¨ê³„ë³„ë¡œ ì‹ ì¤‘í•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
                        """

    prompt = prompt_template.format(
        DATABASE_SCHEMA=schema_info,
        QUESTION=user_question,
        HINT="",  # í•„ìš”ì‹œ íŒíŠ¸ ì¶”ê°€ ê°€ëŠ¥
        FEWSHOTS="",
    )

    response = llm.chat.completions.create(
        model='gpt-4o-mini',
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    generated_text = response.choices[0].message.content
    print("LLM ì‘ë‹µ:\n", generated_text)

    import re

    try:
        # JSON ê°ì²´ ë¸”ë¡ ì¶”ì¶œ ì‹œë„
        match = re.search(r"\{.*\}", generated_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            generated_dict = json.loads(json_str)
            return generated_dict
        else:
            print("JSON í˜•ì‹ ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
    except json.JSONDecodeError as e:
        print("JSON íŒŒì‹± ì‹¤íŒ¨:", e)
        print("íŒŒì‹± ì‹œë„í•œ ë¬¸ìì—´:\n", json_str)
        return None



def find_relevant_columns(query, top_k=7):
    client = chromadb.PersistentClient(path=str(DB_PATH))

    openai_ef = embedding_functions.OpenAIEmbeddingFunction(
        api_key=OPENAI_API_KEY,
        model_name="text-embedding-3-small"
    )

    collection = client.get_or_create_collection("column_description", embedding_function=openai_ef)

    results = collection.query(
        query_texts=[query],
        n_results=top_k
    )

    print(f"\nğŸ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìƒìœ„ {top_k}ê°œ ì»¬ëŸ¼:")

    best_score = 100000
    for doc, meta, score in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):
        print(f"# í…Œì´ë¸”: {meta['table']}, ì»¬ëŸ¼: {meta['column']}")
        print(f"# ì„¤ëª…: {doc.splitlines()[1]}")
        print(f"# ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}\n")
        if score < best_score:
            best_score = score

    print(f"ìµœê³  ì ìˆ˜: {best_score:.4f}")

    return results['metadatas'][0]


def generate_filtered_schema(input_file, used_columns=None, include_descriptions=True):
    with open(input_file, "r", encoding="utf-8") as f:
        tables = json.load(f)

    output_lines = []

    if used_columns:
        used_set = set((item["table"], item["column"]) for item in used_columns)
    else:
        used_set = None

    for table in tables:
        table_name = table["table"]

        if used_set:
            table_columns = [col for col in table["column"] if (table_name, col["column_name"]) in used_set]
            if not table_columns:
                continue
        else:
            table_columns = table["column"]

        output_lines.append(f"í…Œì´ë¸”ëª…: {table_name}")

        for col in table_columns:
            col_name = col["column_name"]
            is_pk = col["PK"] == 1
            fk = col["FK"]
            desc = col.get("description", None)

            if is_pk:
                output_lines.append(f"- ì»¬ëŸ¼ (PK): {col_name}")
            elif fk:
                output_lines.append(f"- ì»¬ëŸ¼ (FK): {col_name}")
            else:
                output_lines.append(f"- ì»¬ëŸ¼: {col_name}")

            if include_descriptions and desc:
                cleaned = desc.strip().lstrip("#").strip()
                output_lines.append(f"  - ì„¤ëª…: {cleaned}")

        output_lines.append("")

    return "\n".join(output_lines)



def get_relevant_fewshots(user_question, top_k=3):
    client = chromadb.PersistentClient(path=str(DB_PATH))
    openai_ef = embedding_functions.OpenAIEmbeddingFunction(
        api_key=OPENAI_API_KEY,
        model_name="text-embedding-3-small"
    )
    collection = client.get_or_create_collection("fewshot", embedding_function=openai_ef)

    results = collection.query(
        query_texts=[user_question],
        n_results=top_k
    )

    fewshot_blocks = []
    for idx, (doc, meta) in enumerate(zip(results["documents"][0], results["metadatas"][0]), 1):
        block = f"""### ì˜ˆì œ {idx}
                        ì§ˆë¬¸: {meta['question']}
                        ê·¼ê±°: {meta['evidence']}
                        SQL:
                        {meta['SQL']}"""
        fewshot_blocks.append(block)

    return "\n\n".join(fewshot_blocks)

#if __name__ == "__main__":
#    question = "2024ë…„ 1í•™ê¸° ìˆ˜í•™ ë™ì•„ë¦¬ì¸ í”¼íƒ€ê³ ë¼ìŠ¤ì— í™œë™í•œ ì´ì›ì • í•™ìƒì˜ í™œë™ì„ ë°”íƒ•ìœ¼ë¡œ ìƒê¸°ë¶€ë¥¼ ì‘ì„±í•´ì¤˜"
#    result = nlTosql(question)
from langchain_community.vectorstores import Chroma  
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

from dotenv import load_dotenv
import os

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

def load_rag_chatbot(persist_path: str):
    embedding = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=openai_api_key,
    )
    vectordb = Chroma(persist_directory=persist_path, embedding_function=embedding)
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})

    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1, openai_api_key=openai_api_key)

    prompt = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ êµìœ¡ë¶€ì˜ í•™êµìƒí™œê¸°ë¡ë¶€ ê¸°ì¬ìš”ë ¹ì„ ì˜ ì•„ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.
    ì•„ë˜ ë¬¸ì„œ ë‚´ìš©(context)ê³¼ ì´ì „ ëŒ€í™”(chat_history)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •ì¤‘í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”.

    - ë¨¼ì € ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ **ì •í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€**ì„ ì œê³µí•˜ì„¸ìš”.
    - ë‹µë³€ì´ ëë‚œ í›„, ë°˜ë“œì‹œ context ì¤‘ ì°¸ì¡°í•œ ë¬¸ì¥ì„ ì•„ë˜ ì¸ìš© í˜•ì‹ìœ¼ë¡œ ë§ë¶™ì´ì„¸ìš”.
    - ì¸ìš©ì€ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:

    ---
    ğŸ“˜ **ì¶œì²˜: 2025 ìƒí™œê¸°ë¡ë¶€ ê¸°ì¬ìš”ë ¹**
    > ë¬¸ì¥ 1  
    > ë¬¸ì¥ 2  
    > ë¬¸ì¥ 3  
    ---

    [ì´ì „ ëŒ€í™”]
    {chat_history}

    [ë¬¸ì„œ ë°œì·Œ]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """)

    memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")

    def format_docs(docs):
        return "\n\n".join([doc.page_content for doc in docs])

    def qa_chain_with_memory(question):
        # ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°
        chat_history = memory.load_memory_variables({}).get("chat_history", "")

        # ë¬¸ì„œ ê²€ìƒ‰ (í•œ ë²ˆë§Œ í˜¸ì¶œ!)
        docs = retriever.get_relevant_documents(question)
        print("docs in module !!")
        print(docs)
        context = format_docs(docs)
        print("context in module!! ")
        print(context)

        # ì²´ì¸ êµ¬ì„± í›„ ì‹¤í–‰
        result = prompt | llm
        response = result.invoke({
            "question": question,
            "context": context,
            "chat_history": chat_history
        })

        # ë©”ëª¨ë¦¬ ì €ì¥
        memory.save_context({"question": question}, {"answer": response.content})

        return {
            "answer": response.content,
            "source_documents": docs
        }

    return qa_chain_with_memory
